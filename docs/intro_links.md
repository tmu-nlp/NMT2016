###NMT実装で使う主なlinksの説明

- EmbedID: one-of-kベクトルを用意しないで（単語idだけで）Embedベクトルをつくれる
- Linear: 入力次元数→出力次元数に変換する線形変換
- LSTM: 
```python
word2emb = EmbedID(V, M) M:入力語彙次元, N:出力次元数
emb2hid = Linear(M, N) M:入力次元数, N:出力次元数
lstm = L.LSTM(M, N) M:x^{t}の次元数, N:出力次元数 ※Mについて、h^{t-1}とc{t}の次元数は考慮しなくてよい

embed_vec = word2emb（入力ベクトル)
hidden_vec = emb2hid(入力ベクトル)
layer = (入力ベクトル)

みたいな使い方
```
（チュートリアルの1章読めばすぐわかるようなことはあまり書いてないです...）

```python
# importするものは以降書きません
import numpy as np
from chainer import Variable
from chainer import links as L
```

####Variableについて
基本的にchainerはVariableというオブジェクト(?)を介してなんかします  
入力も正解ラベルもVariableにします  
重み行列をかませたものもVariableになってます  

```python
x = Variable(np.array([1,2,3,4,5], dtype=np.float32)) # np.arrayかcuda.cupy.arrayのみを引数に取れる
In: x
Out: <variable at 0x1110f3470> # オブジェクト
In: x.data
Out: array([0, 1, 2, 3, 4], dtype=int32) # .dataで中身をみれます
```
#####多くの場合において、Variable.data.shape[0]はbatchサイズ！！！

####EmbedIDについて

#####入力は、基本的にベクトル(!=行列)形式
numpyについて軽くふれておきます
```python
np.array([1,2,3,4,5]) # shape > (5,) これはベクトル
np.array([[1,2,3,4,5]]) # shape > (1,5) これは行列
np.array([[1],
          [2],
          [3],
          [4],
          [5]]) # shape > (5,1) これも行列

ちなみにreshapeで次元数を変更したものを返す
arr = np.array([1,2,3,4,5,6])
reshaped_arr = arr.reshape(3,2) 
In: reshaped_arr
Out: array([[1, 2],
            [3, 4],
            [5, 6]])
```

#####単語idベクトルを用意 
```python
wordid = Variable(np.array([0], dtype=np.int32)) # dtype=np.int32を指定すること!!

# もしbatchサイズ個入力に与えたい場合は、arrayにbatchサイズ個だけidを入れておく
wordid_batch = Variable(np.array([0,3,2,1,5,1], dtype=np.int32))  #Variableへの引数のshapeはベクトル（!=行列）であることに注意

↑ wordid_batchのイメージは以下のようなone-of-kベクトルの集合、
  実際は上記のように単語に対応するidを用意すればよい
In: wordid_batch 
Out: [[1,0,0,0,0,0,0,,,,,], # （batchサイズ,　語彙次元）のone-of-k
      [0,0,0,1,0,0,0,,,,,],
      [0,0,1,0,0,0,0,,,,,],
      [0,1,0,0,0,0,0,,,,,],
      [0,0,0,0,1,0,0,,,,,],
      [0,1,0,0,0,0,0,,,,,]]
```

#####one-of-k → 埋め込み層次元のベクトルに変換
```python
id2emb = L.EmbedID(100,10)
emb = id2emb(wordid) 
In: emb.data
Out: array([[-1.13752449, -0.52425206, -2.9208231 ,  0.94727218, -0.85582888,
             0.81389672,  0.92413682,  1.09788096, -0.10864022,  0.98582453]], dtype=float32) 
             # 100次元のone-of-kベクトル（実際はidを一つ与えているだけ）を10次元のベクトルにして出力

emb = id2emb(wordid_batch)
In: emb.data
Out: array([[-1.13752449, -0.52425206, -2.9208231 ,  0.94727218, -0.85582888,
             0.81389672,  0.92413682,  1.09788096, -0.10864022,  0.98582453],
            [-1.41812336,  0.21968678, -1.03780293,  0.9425984 , -0.49077183,
             -0.53402537, -0.35991779,  1.83347559,  0.41529644, -0.12508786],
            [-0.28689638,  0.27709255,  1.38391078, -0.63276458, -0.41066223,
             0.19682208, -0.33105236,  0.38301867,  0.01794957, -0.2388376 ],
            [-1.68474269, -0.36209288, -0.95555246,  1.55836928, -0.6241619 ,
             0.73265791, -0.36436638, -0.62428874,  1.05810857, -0.31546396],
            [ 0.44774297, -1.03555477, -1.29167831,  1.41903055,  0.61535698,
             0.08044333, -0.91248614,  0.37102258,  0.56521112, -0.47947204],
            [-0.65874404, -0.88339108,  1.11989617,  0.54213691,  0.82275492,
             1.31768203, -0.40221444,  0.18208186,  1.81904447, -0.52305448]], dtype=float32)
              # batchサイズ個のidを入力すると、出力は（batchサイズ, 埋め込み層次元）
```

####Linearについて  

#####線形変換するために用いる

```python
emb2hid = L.Linear(10, 2) # 10次元を2次元に変換
hid = emb2hid(emb) # (batchサイズ, 埋め込み層次元数) * (埋め込み層次元数, 出力次元数) → （batchサイズ, 出力次元数)
In: hid.data
Out: array([[ 1.5764128 ,  0.52369976],
            [ 0.80156308,  0.76217818],
            [-0.0672536 ,  0.54038751],
            [ 0.39135873,  0.54352796],
            [-0.23365495,  0.2514739 ],
            [-1.36178613,  2.24770927]], dtype=float32) 

```
上記のような変換が一般的だけど、他にもこんなことができるよ
```python
x = Variable(np.array(M*N*O, dtype=np.float32).reshape(M,N,O))
In: x.data.shape
Out: (M,N,O) # 

l = L.Linear(N*O, P)
In: l(x).data.shape
Out: (M, P)
```

####LSTMについて

#####linksのLSTMはstatefulとstatelessに分かれるが、使うのはstateful（デフォルトは多分stateful）
#####statefulを用いると、LSTM間の隠れ層ベクトルh^{t-1}と状態ベクトルc^{t}を考慮しなくても勝手に内部でやってくれる
